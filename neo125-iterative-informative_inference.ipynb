{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9bef59-6ceb-46d2-ba0b-a9caaaf72352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from GPTNeoWithIntermediates import GPTNeoWithIntermediates\n",
    "from BatchTokenizer import WikiBatchTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b95474-89b8-4fc4-8ca9-495f721bb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")\n",
    "dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294f21bc-644f-4699-8cda-9f8fafb7a790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPTNeoWithIntermediates.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66acb21-7633-4677-9302-a55750d9a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(sample_count/batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7611f3df-40c3-48a5-b446-2ecb507e13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_inference_with_intermediates(dataset, batch_size, selected_layers):\n",
    "\n",
    "    wiki_tokenizer = WikiBatchTokenizer(dataset=dataset, batch_size=batch_size)\n",
    "    sample_count = wiki_tokenizer.sort_text(ret_len=True)\n",
    "    \n",
    "    for iter in range(2):            # +1 only if you dont get perfect batches.  \n",
    "        tokenized_batch = wiki_tokenizer.gen_batch(iter=iter).to(device)\n",
    "        input_ids, attention_mask = tokenized_batch.input_ids, tokenized_batch.attention_mask\n",
    "\n",
    "        print(tokenized_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predicted_ids = torch.argmax(outputs[\"logits\"], dim=-1)\n",
    "        generated_texts = [wiki_tokenizer.tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "\n",
    "        if selected_layers is None:\n",
    "            selected_layers = list(range(len(outputs[\"layer_outputs\"])))\n",
    "        \n",
    "        selected_outputs = [\n",
    "            {i: layer_output for i, layer_output in enumerate(outputs[\"layer_outputs\"]) if i in selected_layers}\n",
    "            for _ in range(batch_size)\n",
    "        ]\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            print(generated_texts[idx])\n",
    "            print(\" ------------------------------------------------------------- > \\n\")\n",
    "\n",
    "        del outputs\n",
    "        del tokenized_batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    #     return [\n",
    "    #         {\n",
    "    #             \"generated_text\": generated_texts[idx],\n",
    "    #             \"layer_outputs\": selected_outputs[idx],\n",
    "    #     }\n",
    "    #     for idx in range(batch_size)\n",
    "    # ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeda858b-a038-4bb7-b456-03f2fce34f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38f05e2-6093-4097-97c3-0b59c9fbe41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  383,  2615,  2512,   286,  6937,   479, 16628,   318,   262,  2063,\n",
      "          2488,    12,    31,  2665,   366,   406,   366,  3127,   837, 13160,\n",
      "           286,   257,  2168, 47472,  1168,   837,   290,   257,   427,  2797,\n",
      "          6178, 47912,   575,    13,   383,   366,   479,   366,   287,   366,\n",
      "          6937,   479,   366,   318,   262,  1988,  1813,   416,   837,   220,\n",
      "           198, 50256, 50256, 50256, 50256],\n",
      "        [49521, 26615,   373,  4642,   287,  8533,   837,  3936,   319,  2805,\n",
      "           678,   837, 12122,   764,  2399,  3397, 25107,   618,   339,   373,\n",
      "           838,   837,   290,   339,   373,  4376,   416,   465,  2802,   764,\n",
      "           679,  2826,   287,  6205,  5701, 16861,   355,   257,  1200,   837,\n",
      "          1390,  7703,  4041, 17362,   764,   220,   198, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  554,  8275,   837, 46810,  6149,   257,  1862, 13459,   805,   837,\n",
      "          1279,  2954,    29,   837,   284,   307,  3350,  4111,   290,   788,\n",
      "          6405,   683,   764,  4784,   284, 29628, 14154,  3754,   837,  1279,\n",
      "          2954,    29, 18631,   281, 45244, 28204,   284,  9910,  1437,   837,\n",
      "           290, 46810,   772,  1444,   683,   416,   465,  2636,  3656,   705,\n",
      "            82,  1438,   764,   220,   198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      " following was of the-inem is the number-$1-.k- and. which of the single of and and a a seriesunt Zitt Z, The sh half- filter the k k filters filters the same of by the where,\n",
      " ( the ( the\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      "as, a. the, Texas, May 22, 18. He father were in he was a years and he was raised in his mother and He was basketball the sports,, a member, and the League,, He He\n",
      "HeCategoryHeHeHeHeHeCategory\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      " the% the's the new man slave to toi> to to be killed into. killed hanged to. Nero to theysius, Nerounk>, a illegitimate resemblance to Neroine, < Nero was had her \" his first name,the-.\n",
      "\n",
      "\n",
      "\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      "{'input_ids': tensor([[ 1550,  1248,  2901, 16236,   837,   706,   262,  1368,   319,   262,\n",
      "          4141, 20001,   379,   337,   364,  2488,    12,    31,  1288,  2488,\n",
      "            12,    31,   509,  2634,    65,   343,   416,   262,  3517,   837,\n",
      "           262,   569,   488,    88,  1230, 10435,   257, 13471,  9513,   286,\n",
      "         47403,   355,   257,  2882,   764,  7703,  2465,   373,  2098,   284,\n",
      "           423,   587,  1760,   764,   220,   198],\n",
      "        [  554,  5996,   837, 46810,  2540,  2263,   319,   257,   517,  4075,\n",
      "          2597,   355,   281, 18382,   764,   679,   373,   762,   377,  1440,\n",
      "          1661,  1022,  5996,   290,  3126,   764,  5856,   428,  2278,   837,\n",
      "           617,  6156, 21262,  2740,  6547,   880,   286, 46810,   290,  6273,\n",
      "           340,   351,   465,  1568,  3896,   764,   220,   198, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 9754,   262,  1957,  1230,  4991,   837, 31721,   468,   262,  4511,\n",
      "          4466, 20157,   284,  1535,   764,   632,   373,   635,   530,   286,\n",
      "           262,  4736,   351,   262,  4511,  1687,   290,  5387,  6426,   764,\n",
      "          9241,  6426,  5504,   329,  6337,  4064,   286,   262,  1748,   705,\n",
      "            82,  3739,   287,  2321,   764,   220,   198, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}\n",
      " the-,, the the war on the German forces, theentonin The7,--@,-bÃ©, the French, the French-y government decided the naval campaign on the, well result to The is was done, the been done to The\n",
      "\n",
      "\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      " the% the's to the the new serious role in a actor of He was thecript to years in 15 and 60 years He his time, Nero of Roman have of strongly of Nero's his him with the role career.\n",
      "\n",
      "\n",
      ". the the the the the,,\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      " the three and,, the is a highest number deficit of the care\n",
      " is the the of the first in the highest number rate expenditure revenue per\n",
      " revenue is for about% of the total'ss budget, the.\n",
      "\n",
      "\n",
      " the the the the the the the the the\n",
      " ------------------------------------------------------------- > \n",
      "\n",
      "0.7199864387512207\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "iterative_inference_with_intermediates(dataset=dataset, batch_size=3, selected_layers=[6])\n",
    "print(time.time() - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
